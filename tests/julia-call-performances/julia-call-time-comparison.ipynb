{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "root_dir = Path(os.getcwd())\n",
    "\n",
    "if str(root_dir.parents[1]) not in sys.path:\n",
    "  sys.path.append(str(root_dir.parents[1]))\n",
    "\n",
    "from LMORL.BAN.API.agents.DQNHybrid import DQNHybrid\n",
    "from LMORL.BAN.API.ban_utils import Ban\n",
    "\n",
    "import mo_gymnasium as mo_gym\n",
    "\n",
    "env = mo_gym.make(\"LunarLander-v2-mo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_size = env.observation_space.shape[0]\n",
    "num_actions = int(env.action_space.n)\n",
    "action_space = list(range(env.action_space.n))\n",
    "learning_rate = 0.001\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.1\n",
    "batch_size = 64\n",
    "train_start = 64\n",
    "hidden_size = 128\n",
    "BAN_SIZE = 3\n",
    "max_memory_size=100\n",
    "\n",
    "episodes=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09:57:01\tEpisode\t1\ttimesteps:\t76\tTook\t1.966768 sec - reward:\t[-37.464076589499626, -100.0, -7.349999999999999]\t| 100AvgReward: [-37.464076589499626, -100.0, -7.349999999999999]\n",
      "09:57:28\tEpisode\t2\ttimesteps:\t115\tTook\t27.484145 sec - reward:\t[-79.03506463279871, -100.0, -8.88]\t| 100AvgReward: [-58.24957061114917, -100.0, -8.115]\n",
      "09:57:29\tEpisode\t3\ttimesteps:\t78\tTook\t0.737602 sec - reward:\t[83.68559611271071, -100.0, -7.800000000000002]\t| 100AvgReward: [-10.937848369862541, -100.0, -8.01]\n",
      "09:57:30\tEpisode\t4\ttimesteps:\t97\tTook\t1.006529 sec - reward:\t[-64.95950004523328, -100.0, -10.199999999999996]\t| 100AvgReward: [-24.443261288705227, -100.0, -8.5575]\n",
      "09:57:31\tEpisode\t5\ttimesteps:\t69\tTook\t0.674424 sec - reward:\t[99.60739018775139, -100.0, -6.959999999999996]\t| 100AvgReward: [0.36686900658609717, -100.0, -8.237999999999998]\n"
     ]
    }
   ],
   "source": [
    "new_agent = DQNHybrid(input_size=input_size, num_actions=num_actions,\n",
    "                  action_space=action_space, learning_rate=learning_rate,\n",
    "                  epsilon_decay=epsilon_decay, epsilon_min=epsilon_min,\n",
    "                  batch_size=batch_size, hidden_size=hidden_size,\n",
    "                  ban_size=BAN_SIZE, max_memory_size=max_memory_size, train_start=100, use_legacy=False)\n",
    "\n",
    "mname = \"new.model\"\n",
    "\n",
    "rewards, avg_rewards, timings_1, infos_lists = new_agent.learning(env=env,episodes = episodes, mname=mname, replay_frequency=4, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09:58:31\tEpisode\t1\ttimesteps:\t83\tTook\t59.570992 sec - reward:\t[74.43219496351185, -100.0, -7.83]\t| 100AvgReward: [74.43219496351185, -100.0, -7.83]\n",
      "10:00:12\tEpisode\t2\ttimesteps:\t66\tTook\t100.914957 sec - reward:\t[194.09117693971442, -100.0, -4.979999999999999]\t| 100AvgReward: [134.26168595161312, -100.0, -6.404999999999999]\n",
      "10:03:09\tEpisode\t3\ttimesteps:\t66\tTook\t176.131551 sec - reward:\t[102.49043965863622, -100.0, -6.2399999999999975]\t| 100AvgReward: [123.67127052062081, -100.0, -6.349999999999999]\n",
      "10:11:41\tEpisode\t4\ttimesteps:\t128\tTook\t512.185409 sec - reward:\t[-126.9575065699189, -100.0, -11.669999999999991]\t| 100AvgReward: [61.014076247985884, -100.0, -7.679999999999997]\n",
      "10:19:46\tEpisode\t5\ttimesteps:\t90\tTook\t485.463753 sec - reward:\t[-233.43832945894619, -100.0, -9.12]\t| 100AvgReward: [2.1235951065994696, -100.0, -7.967999999999998]\n"
     ]
    }
   ],
   "source": [
    "legacy_agent = DQNHybrid(input_size=input_size, num_actions=num_actions,\n",
    "                  action_space=action_space, learning_rate=learning_rate,\n",
    "                  epsilon_decay=epsilon_decay, epsilon_min=epsilon_min,\n",
    "                  batch_size=batch_size, hidden_size=hidden_size,\n",
    "                  ban_size=BAN_SIZE, max_memory_size=max_memory_size, train_start=100, use_legacy=True)\n",
    "\n",
    "mname = \"legacy.model\"\n",
    "\n",
    "rewards, avg_rewards, timings_2, infos_lists = legacy_agent.learning(env=env,episodes = episodes, mname=mname, replay_frequency=4, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'Ban' has no attribute 'display_execution_time'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# plot execution time\u001b[39;00m\n\u001b[1;32m      2\u001b[0m get_ipython()\u001b[39m.\u001b[39mrun_line_magic(\u001b[39m'\u001b[39m\u001b[39mmatplotlib\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39minline\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m Ban\u001b[39m.\u001b[39;49mdisplay_execution_time(new_timings\u001b[39m=\u001b[39mtimings_1, legacy_timings\u001b[39m=\u001b[39mtimings_2, title\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPyJulia jl_eval and Main object call\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'Ban' has no attribute 'display_execution_time'"
     ]
    }
   ],
   "source": [
    "# plot execution time\n",
    "%matplotlib inline\n",
    "\n",
    "Ban.display_execution_time(new_timings=timings_1, legacy_timings=timings_2, title=\"PyJulia jl_eval and Main object call\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit ('3.8.6')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "36288ad7d93bcd1e894b4a4de2141d1ec27a52c4b002bdac6e1a7b2022682f2c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
