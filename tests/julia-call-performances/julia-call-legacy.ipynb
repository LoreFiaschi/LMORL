{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "root_dir = Path(os.getcwd())\n",
    "\n",
    "if str(root_dir.parents[1]) not in sys.path:\n",
    "  sys.path.append(str(root_dir.parents[1]))\n",
    "\n",
    "from LMORL.BAN.API.agents.DQNHybrid import DQNHybrid\n",
    "from LMORL.BAN.API.ban_utils import Ban\n",
    "\n",
    "import mo_gymnasium as mo_gym\n",
    "\n",
    "env = mo_gym.make(\"LunarLander-v2-mo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_size = env.observation_space.shape[0]\n",
    "num_actions = int(env.action_space.n)\n",
    "action_space = list(range(env.action_space.n))\n",
    "learning_rate = 0.001\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.1\n",
    "batch_size = 64\n",
    "train_start = 64\n",
    "hidden_size = 128\n",
    "BAN_SIZE = 3\n",
    "max_memory_size=100\n",
    "\n",
    "agent = DQNHybrid(input_size=input_size, num_actions=num_actions,\n",
    "                  action_space=action_space, learning_rate=learning_rate,\n",
    "                  epsilon_decay=epsilon_decay, epsilon_min=epsilon_min,\n",
    "                  batch_size=batch_size, hidden_size=hidden_size,\n",
    "                  ban_size=BAN_SIZE, max_memory_size=max_memory_size, train_start=100, use_legacy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:53:11\tEpisode\t1\ttimesteps:\t108\tTook\t111.759528 sec - reward:\t[-155.90095346535736, -100.0, -10.169999999999995]\t| 100AvgReward: [-155.90095346535736, -100.0, -10.169999999999995]\n",
      "10:55:19\tEpisode\t2\ttimesteps:\t66\tTook\t128.108927 sec - reward:\t[52.17021299088006, -100.0, -5.159999999999998]\t| 100AvgReward: [-51.86537023723865, -100.0, -7.6649999999999965]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m episodes \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m\n\u001b[1;32m      2\u001b[0m mname \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlegacy.model\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m rewards, avg_rewards, timings, infos_lists \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mlearning(env\u001b[39m=\u001b[39;49menv,episodes \u001b[39m=\u001b[39;49m episodes, mname\u001b[39m=\u001b[39;49mmname, replay_frequency\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m)\n",
      "File \u001b[0;32m~/GitHubProjects/LMORL/LMORL/BAN/API/agents/agent.py:122\u001b[0m, in \u001b[0;36mAgent.learning\u001b[0;34m(self, env, episodes, mname, replay_frequency, dump_period, reward_threshold, render, verbose)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39m# reward is MO, then it is a list\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[39m# totrew+=reward\u001b[39;00m\n\u001b[1;32m    120\u001b[0m totrew \u001b[39m=\u001b[39m [\u001b[39msum\u001b[39m(foo) \u001b[39mfor\u001b[39;00m foo \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(totrew, reward)]\n\u001b[0;32m--> 122\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_add_experience(state,action_index,reward,next_state,done)\n\u001b[1;32m    123\u001b[0m state\u001b[39m=\u001b[39mnext_state\n\u001b[1;32m    124\u001b[0m t\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/GitHubProjects/LMORL/LMORL/BAN/API/agents/DQNHybrid.py:119\u001b[0m, in \u001b[0;36mDQNHybrid._add_experience\u001b[0;34m(self, state, action_index, reward, next_state, done)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     before \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mnow()\n\u001b[0;32m--> 119\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_main\u001b[39m.\u001b[39;49mstate \u001b[39m=\u001b[39m state\n\u001b[1;32m    120\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_main\u001b[39m.\u001b[39maction_index \u001b[39m=\u001b[39m action_index \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m \u001b[39m#because julia is 1-based, while python is 0-based\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_main\u001b[39m.\u001b[39mreward_list \u001b[39m=\u001b[39m reward\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.6/lib/python3.8/site-packages/julia/core.py:215\u001b[0m, in \u001b[0;36mJuliaMainModule.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m    208\u001b[0m juliapath \u001b[39m=\u001b[39m remove_prefix(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mjulia.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    209\u001b[0m setter \u001b[39m=\u001b[39m \u001b[39m'''\u001b[39m\n\u001b[1;32m    210\u001b[0m \u001b[39mPyCall.pyfunctionret(\u001b[39m\n\u001b[1;32m    211\u001b[0m \u001b[39m    (x) -> Base.eval(\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, :(\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m = $x)),\u001b[39m\n\u001b[1;32m    212\u001b[0m \u001b[39m    Any,\u001b[39m\n\u001b[1;32m    213\u001b[0m \u001b[39m    PyCall.PyAny)\u001b[39m\n\u001b[1;32m    214\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''\u001b[39m\u001b[39m.\u001b[39mformat(juliapath, jl_name(name))\n\u001b[0;32m--> 215\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_julia\u001b[39m.\u001b[39;49meval(setter)(value)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "episodes = 5\n",
    "mname = \"legacy.model\"\n",
    "\n",
    "rewards, avg_rewards, timings, infos_lists = agent.learning(env=env,episodes = episodes, mname=mname, replay_frequency=4, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot execution time\n",
    "\n",
    "def display_execution_time(timings:list, timesteps:int, title:str = \"\"):\n",
    "\n",
    "    tmp=list(zip(*timings))\n",
    "    how_many_components = len(tmp)\n",
    "    \n",
    "    fig, (ax_list) = plt.subplots(how_many_components)\n",
    "\n",
    "    fig.suptitle(title)\n",
    "\n",
    "    plt.show()\n",
    "    return fig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit ('3.8.6')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "36288ad7d93bcd1e894b4a4de2141d1ec27a52c4b002bdac6e1a7b2022682f2c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
